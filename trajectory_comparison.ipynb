{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from umap import UMAP\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters\n",
    "from sklearn.metrics import silhouette_score\n",
    "import glob\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from scipy.stats import mannwhitneyu\n",
    "from figurefirst import mpl_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(data,th):\n",
    "    idx = []\n",
    "    for i in range(len(data)):\n",
    "        if (data[i]>=th):\n",
    "            idx.append(i)\n",
    "    \n",
    "    index = []\n",
    "    for k, g in groupby(enumerate(idx),lambda ix : ix[0] - ix[1]):\n",
    "        index.append((list((map(itemgetter(1), g)))))\n",
    "    return index\n",
    "\n",
    "def scale_data(data):\n",
    "    min_val =  np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return (((data - min_val) / (max_val - min_val)) * 10)\n",
    "\n",
    "def load_and_combine(file_pattern):\n",
    "    \"\"\"Load and combine trajectory files with proper ID handling.\"\"\"\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        df = pd.read_hdf(f)\n",
    "        df = df.iloc[:-1]  # Drop last row\n",
    "        df['trajectory_id'] = f'traj_{i}'\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def load_and_combine_cfd(file_pattern):\n",
    "    \"\"\"Load and combine trajectory files with proper ID handling.\"\"\"\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    dfs = []\n",
    "    for i, f in enumerate(files):\n",
    "        df = pd.read_hdf(f)\n",
    "        df = df.iloc[:-1]  # Drop last row\n",
    "        df['trajectory_id'] = f'traj_{i}'\n",
    "        dfs.append(df)\n",
    "    combined_df=pd.concat(dfs, ignore_index=True)\n",
    "    combined_df['odor']=scale_data(combined_df.odor)\n",
    "    return combined_df\n",
    "\n",
    "def calculate_metrics(X_umap, labels):\n",
    "    \"\"\"Calculate silhouette score and normalized centroid distance.\"\"\"\n",
    "    # Convert string labels to numeric for silhouette score\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    numeric_labels = np.array([label_map[label] for label in labels])\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette = silhouette_score(X_umap, numeric_labels)\n",
    "    \n",
    "    # Calculate centroids\n",
    "    centroids = []\n",
    "    for label in range(len(unique_labels)):\n",
    "        mask = numeric_labels == label\n",
    "        centroid = np.mean(X_umap[mask], axis=0)\n",
    "        centroids.append(centroid)\n",
    "    centroids = np.array(centroids)\n",
    "    \n",
    "    # Calculate distance between centroids\n",
    "    centroid_dist = cdist(centroids, centroids)[0, 1]\n",
    "    \n",
    "    # Calculate average spread within clusters\n",
    "    spreads = []\n",
    "    for label, centroid in enumerate(centroids):\n",
    "        mask = numeric_labels == label\n",
    "        points = X_umap[mask]\n",
    "        spread = np.mean(cdist(points, centroid.reshape(1, -1)))\n",
    "        spreads.append(spread)\n",
    "    \n",
    "    avg_spread = np.mean(spreads)\n",
    "    normalized_dist = centroid_dist / (avg_spread + 1e-10)\n",
    "    \n",
    "    return silhouette, normalized_dist\n",
    "\n",
    "def process_odor_signal(df, threshold=6.5):\n",
    "    \"\"\"Process odor signal to keep only whiff values.\"\"\"\n",
    "    # Create a copy of the odor column\n",
    "    df = df.copy()    \n",
    "    # Process odor by trajectory\n",
    "    for traj_id, traj_df in df.groupby('trajectory_id'):\n",
    "        odor_series = traj_df['odor'].values\n",
    "        \n",
    "        # Get whiff indices\n",
    "        whiff_idx = get_index(odor_series, threshold)\n",
    "        \n",
    "        # Create mask of zeros\n",
    "        processed_odor = np.zeros_like(odor_series)\n",
    "        \n",
    "        # Set whiff values\n",
    "        for idx_group in whiff_idx:\n",
    "            processed_odor[idx_group] = odor_series[idx_group]\n",
    "        \n",
    "        # Update the dataframe\n",
    "        df.loc[traj_df.index, 'odor'] = processed_odor\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryComparison:\n",
    "    def __init__(self, cosmos_df, cfd_df):\n",
    "        self.cosmos_df = cosmos_df\n",
    "        self.cfd_df = cfd_df\n",
    "        self.feature_scaler = None\n",
    "        self.umap_reducer = None\n",
    "        \n",
    "    def normalize_units(self):\n",
    "        \"\"\"Ensure consistent units between datasets\"\"\"\n",
    "        # Scale spatial coordinates to same range\n",
    "        for df in [self.cosmos_df, self.cfd_df]:\n",
    "            # Normalize spatial components\n",
    "            spatial_cols = ['x', 'y', 'vx', 'vy', 'crosswind_dist', 'upwind_dist']\n",
    "            for col in spatial_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col] / df[col].abs().max()\n",
    "            \n",
    "            # Normalize time steps\n",
    "            df['time'] = (df['time'] - df['time'].min()) / (df['time'].max() - df['time'].min())\n",
    "            \n",
    "            # Normalize angles to [-1, 1]\n",
    "            angle_cols = ['heading_angle']\n",
    "            for col in angle_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = np.unwrap(df[col]) % (2 * np.pi)\n",
    "                    df[col] = df[col] / np.pi - 1\n",
    "            \n",
    "            # Normalize other quantities\n",
    "            if 'speed' in df.columns:\n",
    "                df['speed'] = df['speed'] / df['speed'].max()\n",
    "            if 'acceleration' in df.columns:\n",
    "                df['acceleration'] = df['acceleration'] / df['acceleration'].abs().max()\n",
    "            if 'angular_velocity' in df.columns:\n",
    "                df['angular_velocity'] = df['angular_velocity'] / df['angular_velocity'].abs().max()\n",
    "            if 'path_curvature' in df.columns:\n",
    "                df['path_curvature'] = df['path_curvature'] / df['path_curvature'].abs().max()\n",
    "    \n",
    "    def add_derived_features(self, df):\n",
    "        \"\"\"Add carefully normalized derived features\"\"\"\n",
    "        # Simple path efficiency (ratio of displacement to path length)\n",
    "        df['cumulative_dx'] = df.groupby('trajectory_id')['vx'].cumsum()\n",
    "        df['cumulative_dy'] = df.groupby('trajectory_id')['vy'].cumsum()\n",
    "        df['path_length'] = np.sqrt(df['cumulative_dx']**2 + df['cumulative_dy']**2)\n",
    "        df['path_length'] = df['path_length'] / df['path_length'].max()\n",
    "        \n",
    "        # Turn rate (normalized)\n",
    "        df['turn_rate'] = np.gradient(df['heading_angle'], df['time'])\n",
    "        df['turn_rate'] = df['turn_rate'] / df['turn_rate'].abs().max()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_features(self):\n",
    "        \"\"\"Extract features using minimal tsfresh parameters\"\"\"\n",
    "        # Define basic columns to extract features from\n",
    "        base_cols = [\n",
    "            \"vx\", \"vy\", \"speed\", \"acceleration\",\n",
    "            \"heading_angle\", \"angular_velocity\",\n",
    "            \"crosswind_dist\", \"upwind_dist\",\n",
    "            \"path_curvature\", \"turn_rate\",\n",
    "            \"path_length\"\n",
    "        ]\n",
    "        \n",
    "        # Use only columns that exist in both datasets\n",
    "        available_cols = []\n",
    "        for col in base_cols:\n",
    "            if col in self.cosmos_df.columns and col in self.cfd_df.columns:\n",
    "                available_cols.append(col)\n",
    "        \n",
    "        # Use minimal feature set from tsfresh\n",
    "        settings = MinimalFCParameters()\n",
    "        # settings = EfficientFCParameters()\n",
    "        \n",
    "        # Extract features for both datasets\n",
    "        cosmos_features = extract_features(\n",
    "            self.cosmos_df[['trajectory_id', 'time'] + available_cols],\n",
    "            column_id='trajectory_id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=settings\n",
    "        )\n",
    "        \n",
    "        cfd_features = extract_features(\n",
    "            self.cfd_df[['trajectory_id', 'time'] + available_cols],\n",
    "            column_id='trajectory_id',\n",
    "            column_sort='time',\n",
    "            default_fc_parameters=settings\n",
    "        )\n",
    "        \n",
    "        # Ensure both feature sets have the same columns\n",
    "        common_cols = cosmos_features.columns.intersection(cfd_features.columns)\n",
    "        cosmos_features = cosmos_features[common_cols]\n",
    "        cfd_features = cfd_features[common_cols]\n",
    "        \n",
    "        # Balance datasets if needed\n",
    "        min_size = min(len(cosmos_features), len(cfd_features))\n",
    "        if len(cosmos_features) > min_size:\n",
    "            cosmos_features = cosmos_features.sample(n=min_size, random_state=42)\n",
    "        if len(cfd_features) > min_size:\n",
    "            cfd_features = cfd_features.sample(n=min_size, random_state=42)\n",
    "        \n",
    "        return cosmos_features, cfd_features\n",
    "    \n",
    "    def fit_transform(self, n_neighbors=15, min_dist=0.1, metric='euclidean'):\n",
    "        \"\"\"Fit and transform data with careful preprocessing\"\"\"\n",
    "        # Normalize units\n",
    "        self.normalize_units()\n",
    "        \n",
    "        # Add derived features\n",
    "        self.cosmos_df = self.add_derived_features(self.cosmos_df)\n",
    "        self.cfd_df = self.add_derived_features(self.cfd_df)\n",
    "        \n",
    "        # Extract features\n",
    "        cosmos_features, cfd_features = self.extract_features()\n",
    "        \n",
    "        # print(\"Number of features:\", len(cosmos_features.columns))\n",
    "        # print(\"\\nFeature names:\")\n",
    "        # for col in cosmos_features.columns:\n",
    "        #     print(col)\n",
    "\n",
    "\n",
    "        \n",
    "        # Combine features\n",
    "        X = pd.concat([cosmos_features, cfd_features])\n",
    "        y = np.concatenate([\n",
    "            np.zeros(len(cosmos_features)),\n",
    "            np.ones(len(cfd_features))\n",
    "        ])\n",
    "        \n",
    "        # Scale features\n",
    "        self.feature_scaler = RobustScaler()\n",
    "        X_scaled = self.feature_scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit UMAP\n",
    "        self.umap_reducer = UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=min_dist,\n",
    "            metric=metric,\n",
    "            random_state=42\n",
    "        )\n",
    "        X_umap = self.umap_reducer.fit_transform(X_scaled)\n",
    "        \n",
    "        return X_umap, y\n",
    "        \n",
    "    def analyze_temporal_correlation(self):\n",
    "        \"\"\"Analyze temporal correlation between odor encounters and movement patterns.\"\"\"\n",
    "        def compute_windowed_correlation(odor, velocity, window_size=100):\n",
    "            T = len(odor)\n",
    "            correlation = np.zeros(T)\n",
    "            \n",
    "            # Standardize signals\n",
    "            odor_std = (odor - np.mean(odor)) / np.std(odor)\n",
    "            vel_std = (velocity - np.mean(velocity)) / np.std(velocity)\n",
    "            \n",
    "            # Compute windowed correlation\n",
    "            for t in range(T - window_size):\n",
    "                odor_window = odor_std[t:t+window_size]\n",
    "                vel_window = vel_std[t:t+window_size]\n",
    "                correlation[t] = np.sum(odor_window * vel_window) / window_size\n",
    "                \n",
    "            return correlation\n",
    "\n",
    "        # Compute correlations for both simulators\n",
    "        cosmos_correlations = []\n",
    "        cfd_correlations = []\n",
    "        \n",
    "        # Process COSMOS trajectories\n",
    "        for traj_id in self.cosmos_df['trajectory_id'].unique():\n",
    "            traj = self.cosmos_df[self.cosmos_df['trajectory_id'] == traj_id]\n",
    "            odor = traj['odor'].values\n",
    "            velocity = np.sqrt(traj['vx']**2 + traj['vy']**2).values\n",
    "            corr = compute_windowed_correlation(odor, velocity)\n",
    "            cosmos_correlations.append(np.mean(corr))\n",
    "        \n",
    "        # Process CFD trajectories\n",
    "        for traj_id in self.cfd_df['trajectory_id'].unique():\n",
    "            traj = self.cfd_df[self.cfd_df['trajectory_id'] == traj_id]\n",
    "            odor = traj['odor'].values\n",
    "            velocity = np.sqrt(traj['vx']**2 + traj['vy']**2).values\n",
    "            corr = compute_windowed_correlation(odor, velocity)\n",
    "            cfd_correlations.append(np.mean(corr))\n",
    "        \n",
    "        # Statistical test\n",
    "        statistic, pvalue = mannwhitneyu(cosmos_correlations, cfd_correlations)\n",
    "        \n",
    "        return {\n",
    "            'cosmos_correlations': np.array(cosmos_correlations),\n",
    "            'cfd_correlations': np.array(cfd_correlations),\n",
    "            'p_value': pvalue\n",
    "        }\n",
    "    \n",
    "    def plot_comparison(self, X_umap, labels, silhouette, norm_dist):\n",
    "        \"\"\"Plot comparison with enhanced visualization and axis labels.\"\"\"\n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        \n",
    "        # Plot points\n",
    "        for i, label in enumerate(['COSMOS', 'CFD']):\n",
    "            mask = labels == i\n",
    "            ax.scatter(\n",
    "                X_umap[mask, 0],\n",
    "                X_umap[mask, 1],\n",
    "                label=label,\n",
    "                alpha=0.6,\n",
    "                s=50,\n",
    "                c=['#bc141a', '#1764ab'][i]\n",
    "            )\n",
    "        \n",
    "        # Calculate variance ratios directly from UMAP coordinates\n",
    "        # var_ratio = np.var(X_umap, axis=0)\n",
    "        # var_ratio = (var_ratio / np.sum(var_ratio)) * 100\n",
    "        \n",
    "        # # Add labels with variance percentages\n",
    "        # ax.set_xlabel(f'UMAP 1 ({var_ratio[0]:.1f}% var)')\n",
    "        # ax.set_ylabel(f'UMAP 2 ({var_ratio[1]:.1f}% var)')\n",
    "        \n",
    "        ax.set_xlabel('UMAP 1')\n",
    "        ax.set_ylabel('UMAP 2')\n",
    "        mpl_functions.adjust_spines(ax,['left', 'bottom'],\n",
    "                           spine_locations={'left': 5, 'bottom': 5}, \n",
    "                           smart_bounds=True,\n",
    "                           yticks=[-4,8],\n",
    "                           xticks=[-5,15],\n",
    "                           linewidth=0.5)\n",
    "\n",
    "        ax.set_title(f'$a_s$={silhouette:.2f} $d_{{norm}}$={norm_dist:.2f}')\n",
    "        ax.legend()\n",
    "        mpl_functions.set_fontsize(ax, 14)\n",
    "        f.tight_layout()\n",
    "        \n",
    "        return f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"data/tracking/trajectories/\"\n",
    "    osdf_combined = load_and_combine(str(path)+\"cosmos*.h5\")\n",
    "    cfd_df_combined = load_and_combine_cfd(str(path)+\"cfd*.h5\")\n",
    "    \n",
    "    # Create comparison object\n",
    "    comparator = TrajectoryComparison(osdf_combined, cfd_df_combined)\n",
    "    X_umap, labels = comparator.fit_transform(\n",
    "        n_neighbors=50,  # \n",
    "        min_dist=0.1,    \n",
    "        metric='euclidean'  \n",
    "    )\n",
    "    \n",
    "    silhouette, norm_dist = calculate_metrics(X_umap, labels)\n",
    "    comparator.plot_comparison(X_umap, labels, silhouette, norm_dist)\n",
    "\n",
    "    print(\"\\nCluster Quality Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "    print(f\"Normalized Centroid Distance: {norm_dist:.3f}\")\n",
    "    \n",
    "# Analyze temporal correlations\n",
    "    # correlation_results = comparator.analyze_temporal_correlation()\n",
    "\n",
    "    # print(f\"Correlation Analysis p-value: {correlation_results['p_value']:.3f}\")\n",
    "\n",
    "    # # Optional: Plot correlation distributions\n",
    "    # plt.figure(figsize=(6, 4))\n",
    "    # plt.boxplot([correlation_results['cosmos_correlations'], \n",
    "    #             correlation_results['cfd_correlations']],\n",
    "    #             labels=['COSMOS', 'CFD'])\n",
    "    # plt.ylabel('Odor-Motion Correlation')\n",
    "    # plt.title('Temporal Correlation Comparison')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode38kernel",
   "language": "python",
   "name": "pywork38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
